{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 표제어 추출\n",
    "\n",
    "표제어(Lemmatization) 추출 : 서로 형태는 다르지만, root 단어를 가지고 비교해서, 전체적으로 단어의 개수를 줄이자.\n",
    "\n",
    "am, are, is, was, were => be(표제어)\n",
    "\n",
    "형태소 : stem (어간, 단어의 의미), affix(접사, 부가적 의미)\n",
    "어간, 접사를 분리하는 작업\n",
    "\n",
    "dog(독립형태소)\n",
    "dogs = dog(어간) + s(접사)\n",
    "\n",
    "WordNetLemmatizer : NLTK의 표제어 추출 도구\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "#               단어      품사\n",
    "wnl.lemmatize('watched', 'v')# watched\n",
    "wnl.lemmatize('has', 'v')    # have\n",
    "wnl.lemmatize('was', 'v')    # be\n",
    "wnl.lemmatize('gone', 'v')   # go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Python is an interpreted, high-level, general-purpose programming language.'\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Python', 'python'), ('is', 'is'), ('an', 'an'), ('interpreted', 'interpret'), (',', ','), ('high-level', 'high-level'), (',', ','), ('general-purpose', 'general-purpos'), ('programming', 'program'), ('language', 'languag'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "change=[]\n",
    "for i in words:\n",
    "    change.append((i,ps.stem(i)))\n",
    "\n",
    "print(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electric\n",
      "formal\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('electricical'))  # electric\n",
    "print(ps.stem('formalize'))  # formal\n",
    "\n",
    "# 구글에 마틴포터 or 포터스태머 검색 (영문 스태밍)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "gone\n",
      "going\n",
      "gon\n",
      "die\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('going'))\n",
    "print(ps.stem('gone'))\n",
    "\n",
    "# 다른 stemming\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "print(ls.stem('going'))\n",
    "print(ls.stem('gone'))\n",
    "print(ls.stem('dies'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불용어 (stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Family', 'not', 'important', 'thing', '.', 'It', \"'s\", 'everything']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "\n",
    "ex = \"Family is not an important thing. It's everything\"\n",
    "\n",
    "# 토큰화\n",
    "wt = word_tokenize(ex)\n",
    "\n",
    "for i in wt:\n",
    "    if i in sw:\n",
    "        wt.remove(i)\n",
    "\n",
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ranks.nl/stopwords/korean : 한글 불용어 사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['최근', '코로나19로', '인한', '감염으로', '인해', '확진자', '및', '사망자가', '증가하고', '있습니다', '.', '코로나', '19를', '이겨냅시다', '.']\n",
      "['코로나19로', '감염으로', '인해', '확진자', '사망자가', '증가하고', '있습니다', '.', '코로나', '19를', '이겨냅시다', '.']\n"
     ]
    }
   ],
   "source": [
    "ex = '최근 코로나19로 인한 감염으로 인해 확진자 및 사망자가 증가하고 있습니다. 코로나 19를 이겨냅시다.'\n",
    "\n",
    "# 불용어 사전 생성\n",
    "stop_words = '인한 증가 최근 및'\n",
    "stop_words = stop_words.split(' ')\n",
    "\n",
    "# 단어 토큰화\n",
    "wt = word_tokenize(ex)\n",
    "\n",
    "print(wt)\n",
    "\n",
    "# 불용어 제거\n",
    "res = []\n",
    "for w in wt:\n",
    "    if w not in stop_words:\n",
    "        res.append(w)\n",
    "        \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['python', 'interpreted', 'high-level', 'general-purpose', 'programming', 'language'], ['created', 'guido', 'van', 'rossum', 'first', 'released', '1991', 'python', 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'notable', 'use', 'significant', 'whitespace'], ['language', 'constructs', 'object-oriented', 'approach', 'aim', 'help', 'programmers', 'write', 'clear', 'logical', 'code', 'small', 'large-scale', 'projects']]\n",
      "['language', 'constructs', 'object-oriented', 'approach', 'aim', 'help', 'programmers', 'write', 'clear', 'logical', 'code', 'small', 'large-scale', 'projects']\n"
     ]
    }
   ],
   "source": [
    "text = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n",
    "\n",
    "# 문장단위 토큰화\n",
    "text = sent_tokenize(text)  # 3개의 문장\n",
    "text\n",
    "\n",
    "# 단어단위 토큰화\n",
    "# 모든 단어를 소문자, 불용어 제거, 길이가 2이하인 단어 제거\n",
    "res = []\n",
    "\n",
    "for i in text:\n",
    "    r = []\n",
    "    words = word_tokenize(i)\n",
    "    for w in words:\n",
    "        word = w.lower()\n",
    "        if (word not in sw) & (len(word)>2):\n",
    "            r.append(word)\n",
    "    res.append(r)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['python', 'interpreted', 'high-level', 'general-purpose', 'programming', 'language'], ['created', 'guido', 'van', 'rossum', 'first', 'released', '1991', 'python', 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'notable', 'use', 'significant', 'whitespace'], ['language', 'constructs', 'object-oriented', 'approach', 'aim', 'help', 'programmers', 'write', 'clear', 'logical', 'code', 'small', 'large-scale', 'projects']]\n"
     ]
    }
   ],
   "source": [
    "text = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n",
    "\n",
    "# 문장단위 토큰화\n",
    "text = sent_tokenize(text)  # 3개의 문장\n",
    "text\n",
    "\n",
    "# 단어단위 토큰화\n",
    "# 모든 단어를 소문자, 불용어 제거, 길이가 2이하인 단어 제거\n",
    "res = []\n",
    "voc = {}\n",
    "sentences = []\n",
    "\n",
    "for i in text:\n",
    "    res = []\n",
    "    words = word_tokenize(i)\n",
    "    for w in words:\n",
    "        word = w.lower()\n",
    "        # 어근 제거\n",
    "        # word = ps.stem(word)\n",
    "        if (word not in sw) & (len(word)>2):\n",
    "            res.append(word)\n",
    "            # 딕셔너리의 단어 출현 빈도 추가\n",
    "            if word not in voc:\n",
    "                voc[word] = 0\n",
    "            voc[word] += 1\n",
    "    sentences.append(res)\n",
    "\n",
    "# print(res)\n",
    "# print(voc)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(voc.items(), key= lambda x:x[0])  # key 기준 오름차순 정렬\n",
    "vs = sorted(voc.items(), key= lambda x:x[1], reverse=True)  # value 기준 내림차순 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': 1, 'language': 2, 'code': 3}\n"
     ]
    }
   ],
   "source": [
    "wi = {}\n",
    "idx = 0 \n",
    "\n",
    "# 2번이상 언급된 단어 딕셔너리 생성\n",
    "for w, f in vs:\n",
    "    if f > 1 :\n",
    "        idx+=1\n",
    "        wi[w] = idx # 인덱스 부여\n",
    "print(wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocSize = 2  # 가장 많이 언급 된 2개의 단어만 추출\n",
    "# 인덱스가 3번 이상인 단어는 제거(1번, 2번만 남김)\n",
    "\n",
    "# 단어의 인덱스가 vocSize를 초과하는 단어 추출\n",
    "wordFreq = [w for w, i in wi.items() if i > vocSize]\n",
    "wordFreq\n",
    "\n",
    "# 초과하는 단어 제거\n",
    "for w in wordFreq:\n",
    "    # 바로 제거됨\n",
    "    del wi[w]\n",
    "    \n",
    "wordFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOV (out of voc) : 단어집합에 없는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n영수 : 철수야 안녕?  (입력 데이터, X)\\n철수 : 응 너도 안녕. (출력 데이터, Y)\\n\\n철수야 안녕? => 모델 => 응 너도 안녕.\\n'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "영수 : 철수야 안녕?  (입력 데이터, X)\n",
    "철수 : 응 너도 안녕. (출력 데이터, Y)\n",
    "              \n",
    "              수치화\n",
    "철수야 안녕? => 모델 => 응 너도 안녕.\n",
    "\n",
    "철수 안녕   => 모델  => 응 너 안녕\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫인코딩\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '는', '자연어', '처리', '를', '학습', '한다', '.']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 형태소로 나누기\n",
    "tok = okt.morphs('나는 자연어처리를 학습한다.')\n",
    "tok\n",
    "# 원핫벡터 : 단어 집합을 벡터로 표현하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '학습': 5, '한다': 6, '.': 7}\n"
     ]
    }
   ],
   "source": [
    "w2i = {}\n",
    "\n",
    "for v in tok:\n",
    "#     print(v)\n",
    "    if v not in w2i.keys():\n",
    "        w2i[v] = len(w2i)\n",
    "print(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자연어 -> 원핫 -> 0010000\n",
    "def ohe(w, w2i):\n",
    "    \n",
    "    # 리스트 초기화\n",
    "    ohv = [0] * len(w2i)\n",
    "    \n",
    "    # 해당 단어의 인덱스 추출\n",
    "    idx = w2i[w]\n",
    "    \n",
    "    # 리스트의 인덱스 위치를 1로 변경\n",
    "    ohv[idx] = 1\n",
    "    \n",
    "    return ohv\n",
    "\n",
    "ohe('자연어', w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 원핫인코딩\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "tok = Tokenizer()\n",
    "\n",
    "text = '데이터 분석은 판다스 최고야 판다스 곰이야'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.fit_on_texts([text])\n",
    "\n",
    "# 공백으로 구분된 단어별 인덱스 딕셔너리가 생성\n",
    "tok.word_index  # 단어집합 (VOC)\n",
    "# {'판다스': 1, '데이터': 2, '분석은': 3, '최고야': 4, '곰이야': 5}\n",
    "\n",
    "sample = '판다스 분석은 동물원에서 한다'\n",
    "\n",
    "# 단어집합에 있는 단어들의 인덱스\n",
    "enc = tok.texts_to_sequences([sample])[0]  # [1, 3]\n",
    "\n",
    "# voc에 있는 단어들 원핫 인코딩\n",
    "to_categorical(enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE : byte pair encoding (단어분리) => 기계번역\n",
    "학습과정에서 사용되지 않은 단어가 테스트 과정에서 입력되면 -> OOV 문제발생 => 모델이 제대로 동작 x\n",
    "\n",
    "run-length 기법  (비트맵 이미지)\n",
    ": aaaabbbaaaa => a4b3a5\n",
    "\n",
    "허프만 트리를 이용한 압축  (jpeg)\n",
    ": a=101, b=10, c=1101 ... \n",
    "\n",
    "BPE 압축 알고리즘 기법 => 단어분리에 응용\n",
    ": AAABDAAABAC => xDxAC\n",
    "연속적인 글자 쌍(2글자)을 구성했을 때 가장 많이 등장한 쌍\n",
    "1) AA가 가장 많이 등장 => z로 치환\n",
    " zABDzABAC\n",
    " \n",
    "2) AB가 가장 많이 등장 => y로 치환\n",
    " zyDzyAC\n",
    "\n",
    "3) zy가 가장 많이 등장 => x로 치환\n",
    " xDxAC\n",
    " \n",
    " \n",
    " BPE : 단어분리 알고리즘 \n",
    "  => 글자단위\n",
    "  \n",
    " 1) 단어집합 (VOC)\n",
    "  => 'low, lower, newest, widest'\n",
    " ex) lowesr 입력 => OOV에 문제\n",
    "            | 해결\n",
    "            V\n",
    "2) BPE 알고리즘\n",
    "{low:5, lower:2, newest:6, widest:3}\n",
    "  1. 알파벳단위로 분리 l, o, w, e, r, n, w, s, t, i, d\n",
    "      (N-gram) lo:7, ow:7, we:8, .. es:9 .. \n",
    "  2. 가장 빈번 (es)                                                 es를 한 글자로 처리\n",
    "      {low:5, lower:2, newest:6, widest:3} => {low:5, lower:2, new(es)t:6, wid(es)t:3}\n",
    "  3. 빈도수 높은 쌍 다시 찾기 ((es)t)가 9쌍으로 최빈\n",
    "      {low:5, lower:2, new(es)t:6, wid(es)t:3} => {low:5, lower:2, new(est):6, wid(est):3}\n",
    "  4. 빈도수 높은 쌍 다시 찾기 (lo)가 7쌍으로 최빈\n",
    "      {low:5, lower:2, new(est):6, wid(est):3} => {(lo)w:5, (lo)wer:2, new(est):6, wid(est):3}\n",
    "  5. 빈도수 높은 쌍 다시 찾기 ((lo)w)가 7쌍으로 최빈\n",
    "      (lo)w:5, (lo)wer:2, new(est):6, wid(est):3} => (low):5, (low)er:2, new(est):6, wid(est):3}\n",
    "                                    유일한 쌍이 나올 때 까지 반복\n",
    "  n회 반복\n",
    "  l, o, w, e, r, n, w, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest\n",
    "                     ^\n",
    "                     |  참조\n",
    "  1. 테스트 과정에서 'lowest' 입력\n",
    "  2. 글자단위로 분할 l, o, w, e, s, t\n",
    "  3. low / est 로 분할하여 확인하면 OOV가 아니다 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 언어 모델? 언어를 모델링(단어 순서에 대한 확률)\n",
    " => 통계 : 베이즈이론 (조건부확률) p(이전단어|다음단어)\n",
    " => 인공신경망 : \n",
    "                <  <  ngram  >  >\n",
    "    ex) CBOW 나는 오늘 (    ) 타고 집에 갑니다.\n",
    "    \n",
    "                    >  >  ngram  <  <\n",
    "    ex) Skipgram 나는 오늘 (    ) 타고 집에 갑니다.\n",
    "    \n",
    " = > 기계번역\n",
    "             O                   X\n",
    "     p(나는 전철을 탔다) > p(나는 전철을 태운다) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽모델링 => LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "dataser = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers','fotters','quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataser\n",
    "documents = dataset.data\n",
    "len(documents)  # 11314건의 뉴스기사\n",
    "\n",
    "type(documents)  # list\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names  # 20개의 뉴스의 카테고리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can\\'t pity you, Jim.  And I\\'m sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won\\'t be bummin\\' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don\\'t forget your Flintstone\\'s Chewables!  :) \\n--\\nBake Timmons, III\\n\\n-- \"...there\\'s nothing higher, stronger, more wholesome and more useful in life\\nthan some good memory...\" -- Alyosha in Brothers Karamazov (Dostoevsky)\\n'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]  # 필요없는 문자가 상당히 많음 => 정규표현식으로 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic1 토픽별 가장 관련성이 높은 단어를 10개씩 출력\n",
    "~ \n",
    "topic20 \n",
    "으로 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\\n   Although I realize that principle is not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\\n    Notwithstanding all the legitimate fuss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>\\n\\n\\nAn apt description of the content of jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               documents\n",
       "0      Well i'm not sure about the story nad it did s...\n",
       "1      \\n\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to ...\n",
       "2      \\n   Although I realize that principle is not ...\n",
       "3      \\n    Notwithstanding all the legitimate fuss ...\n",
       "4      Well, I will have to change the scoring on my ...\n",
       "...                                                  ...\n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...\n",
       "11310  \\n\\n\\nAn apt description of the content of jus...\n",
       "11311  \\nI agree.  Home runs off Clemens are always m...\n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...\n",
       "11313                                        ^^^^^^\\n...\n",
       "\n",
       "[11314 rows x 1 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsDf = pd.DataFrame({'documents':documents})\n",
    "newsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>Well i m not sure about the story nad it did s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to ...</td>\n",
       "      <td>Yeah  do you expect people to read the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\\n   Although I realize that principle is not ...</td>\n",
       "      <td>Although I realize that principle is not o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\\n    Notwithstanding all the legitimate fuss ...</td>\n",
       "      <td>Notwithstanding all the legitimate fuss a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>Well  I will have to change the scoring on my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>Danny Rubenstein  an Israeli journalist  will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>\\n\\n\\nAn apt description of the content of jus...</td>\n",
       "      <td>An apt description of the content of just a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>I agree   Home runs off Clemens are always me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               documents  \\\n",
       "0      Well i'm not sure about the story nad it did s...   \n",
       "1      \\n\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to ...   \n",
       "2      \\n   Although I realize that principle is not ...   \n",
       "3      \\n    Notwithstanding all the legitimate fuss ...   \n",
       "4      Well, I will have to change the scoring on my ...   \n",
       "...                                                  ...   \n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...   \n",
       "11310  \\n\\n\\nAn apt description of the content of jus...   \n",
       "11311  \\nI agree.  Home runs off Clemens are always m...   \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...   \n",
       "11313                                        ^^^^^^\\n...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      Well i m not sure about the story nad it did s...  \n",
       "1              Yeah  do you expect people to read the...  \n",
       "2          Although I realize that principle is not o...  \n",
       "3           Notwithstanding all the legitimate fuss a...  \n",
       "4      Well  I will have to change the scoring on my ...  \n",
       "...                                                  ...  \n",
       "11309  Danny Rubenstein  an Israeli journalist  will ...  \n",
       "11310     An apt description of the content of just a...  \n",
       "11311   I agree   Home runs off Clemens are always me...  \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...  \n",
       "11313                                               N...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsDf['clean_doc'] = newsDf['documents'].str.replace('[^a-zA-Z ]',' ')\n",
    "newsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3글자 이하 제거\n",
    "newsDf['clean_doc'] = newsDf['clean_doc'].apply(lambda x:' '.join([w for w in x.split(' ') if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대문자를 소문자로\n",
    "newsDf['clean_doc'] = newsDf['clean_doc'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 (공백을 기준으로 토큰화)\n",
    "tokenizedDoc = newsDf['clean_doc'].apply(lambda x:x.split())\n",
    "\n",
    "# 불용어 제거\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "tokenizedDoc = tokenizedDoc.apply(lambda x : [item for item in x if item not in sw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure story seem biased disagree statement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to ...</td>\n",
       "      <td>yeah expect people read actually accept hard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\\n   Although I realize that principle is not ...</td>\n",
       "      <td>although realize principle strongest points wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\\n    Notwithstanding all the legitimate fuss ...</td>\n",
       "      <td>notwithstanding legitimate fuss proposal much ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well change scoring playoff pool unfortunately...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>danny rubenstein israeli journalist speaking t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>\\n\\n\\nAn apt description of the content of jus...</td>\n",
       "      <td>description content ronroth posts date least e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>agree home runs clemens always memorable kinda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>used deskjet orange micros grappler system upd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>argument murphy scared hell came last year han...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               documents  \\\n",
       "0      Well i'm not sure about the story nad it did s...   \n",
       "1      \\n\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to ...   \n",
       "2      \\n   Although I realize that principle is not ...   \n",
       "3      \\n    Notwithstanding all the legitimate fuss ...   \n",
       "4      Well, I will have to change the scoring on my ...   \n",
       "...                                                  ...   \n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...   \n",
       "11310  \\n\\n\\nAn apt description of the content of jus...   \n",
       "11311  \\nI agree.  Home runs off Clemens are always m...   \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...   \n",
       "11313                                        ^^^^^^\\n...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      well sure story seem biased disagree statement...  \n",
       "1      yeah expect people read actually accept hard a...  \n",
       "2      although realize principle strongest points wo...  \n",
       "3      notwithstanding legitimate fuss proposal much ...  \n",
       "4      well change scoring playoff pool unfortunately...  \n",
       "...                                                  ...  \n",
       "11309  danny rubenstein israeli journalist speaking t...  \n",
       "11310  description content ronroth posts date least e...  \n",
       "11311  agree home runs clemens always memorable kinda...  \n",
       "11312  used deskjet orange micros grappler system upd...  \n",
       "11313  argument murphy scared hell came last year han...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF 매트릭스 구성\n",
    "# tfidf는 토큰화가 되지 않은 텍스트 데이터로 구성\n",
    "\n",
    "# 역토큰화\n",
    "# tokenizedDoc.apply(lambda x : ' '.join(x))  # 내 코드\n",
    "\n",
    "deTokenizedDoc = []\n",
    "\n",
    "for i in range(len(newsDf)) :\n",
    "    temp = ' '.join(tokenizedDoc[i])\n",
    "    deTokenizedDoc.append(temp)\n",
    "\n",
    "newsDf['clean_doc'] = deTokenizedDoc\n",
    "newsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf행렬 구성\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vector = TfidfVectorizer(stop_words='english',\n",
    "               max_features=1000)  # 1,000개의 단어만 사용해서 구성하겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = vector.fit_transform(newsDf['clean_doc'])\n",
    "res.shape  # (11314, 1000)  // 11314개의 문서 1000개의 단어 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd (full, truncated) : 특이값 분해\n",
    "# 행렬  = U * s * VT\n",
    "\n",
    "# 절단된 SVD -> 차원 축소\n",
    "# 토픽 숫자 : n_components\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svdModel = TruncatedSVD(n_components=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "svdModel.fit(res)\n",
    "\n",
    "np.shape(svdModel.components_)  # VT = 20, 1000  // 20개의 토픽, 1000개의 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'able',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'according',\n",
       " 'account',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'actually',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'address',\n",
       " 'administration',\n",
       " 'advance',\n",
       " 'advice',\n",
       " 'agencies',\n",
       " 'agree',\n",
       " 'algorithm',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allows',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'analysis',\n",
       " 'andrew',\n",
       " 'angeles',\n",
       " 'anonymous',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'anti',\n",
       " 'anybody',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appears',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'apply',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'april',\n",
       " 'arab',\n",
       " 'archive',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'armenia',\n",
       " 'armenian',\n",
       " 'armenians',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'assume',\n",
       " 'atheism',\n",
       " 'atheists',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'away',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'belief',\n",
       " 'beliefs',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bible',\n",
       " 'bike',\n",
       " 'bios',\n",
       " 'bitnet',\n",
       " 'bits',\n",
       " 'black',\n",
       " 'block',\n",
       " 'blood',\n",
       " 'blue',\n",
       " 'board',\n",
       " 'body',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boston',\n",
       " 'bought',\n",
       " 'break',\n",
       " 'brian',\n",
       " 'bring',\n",
       " 'brought',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'business',\n",
       " 'cable',\n",
       " 'california',\n",
       " 'called',\n",
       " 'calls',\n",
       " 'came',\n",
       " 'canada',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'carry',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cause',\n",
       " 'center',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'chicago',\n",
       " 'child',\n",
       " 'children',\n",
       " 'chip',\n",
       " 'chips',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'chris',\n",
       " 'christ',\n",
       " 'christian',\n",
       " 'christianity',\n",
       " 'christians',\n",
       " 'church',\n",
       " 'citizens',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'claim',\n",
       " 'claims',\n",
       " 'class',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'clinton',\n",
       " 'clipper',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'code',\n",
       " 'college',\n",
       " 'color',\n",
       " 'colorado',\n",
       " 'colors',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'command',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'committee',\n",
       " 'common',\n",
       " 'communications',\n",
       " 'community',\n",
       " 'comp',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'computer',\n",
       " 'conclusion',\n",
       " 'condition',\n",
       " 'conference',\n",
       " 'congress',\n",
       " 'consider',\n",
       " 'considered',\n",
       " 'contact',\n",
       " 'contains',\n",
       " 'context',\n",
       " 'continue',\n",
       " 'control',\n",
       " 'controller',\n",
       " 'convert',\n",
       " 'copy',\n",
       " 'correct',\n",
       " 'cost',\n",
       " 'costs',\n",
       " 'count',\n",
       " 'countries',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'create',\n",
       " 'created',\n",
       " 'crime',\n",
       " 'cross',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'data',\n",
       " 'date',\n",
       " 'dave',\n",
       " 'david',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'default',\n",
       " 'defense',\n",
       " 'define',\n",
       " 'deleted',\n",
       " 'department',\n",
       " 'dept',\n",
       " 'described',\n",
       " 'design',\n",
       " 'designed',\n",
       " 'details',\n",
       " 'detroit',\n",
       " 'developed',\n",
       " 'development',\n",
       " 'device',\n",
       " 'devices',\n",
       " 'died',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'digital',\n",
       " 'direct',\n",
       " 'directly',\n",
       " 'directory',\n",
       " 'disclaimer',\n",
       " 'discussion',\n",
       " 'disease',\n",
       " 'disk',\n",
       " 'disks',\n",
       " 'display',\n",
       " 'distribution',\n",
       " 'division',\n",
       " 'doctor',\n",
       " 'domain',\n",
       " 'door',\n",
       " 'double',\n",
       " 'doubt',\n",
       " 'draw',\n",
       " 'drive',\n",
       " 'driver',\n",
       " 'drivers',\n",
       " 'drives',\n",
       " 'drug',\n",
       " 'drugs',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earth',\n",
       " 'easily',\n",
       " 'east',\n",
       " 'easy',\n",
       " 'education',\n",
       " 'effect',\n",
       " 'effective',\n",
       " 'effort',\n",
       " 'electronic',\n",
       " 'email',\n",
       " 'encryption',\n",
       " 'enforcement',\n",
       " 'engine',\n",
       " 'engineering',\n",
       " 'entire',\n",
       " 'entry',\n",
       " 'environment',\n",
       " 'equipment',\n",
       " 'eric',\n",
       " 'error',\n",
       " 'errors',\n",
       " 'escrow',\n",
       " 'especially',\n",
       " 'event',\n",
       " 'events',\n",
       " 'evidence',\n",
       " 'evil',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'excellent',\n",
       " 'exist',\n",
       " 'existence',\n",
       " 'exists',\n",
       " 'expect',\n",
       " 'expected',\n",
       " 'expensive',\n",
       " 'experience',\n",
       " 'explain',\n",
       " 'export',\n",
       " 'extra',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'failed',\n",
       " 'fairly',\n",
       " 'faith',\n",
       " 'fall',\n",
       " 'false',\n",
       " 'family',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'father',\n",
       " 'features',\n",
       " 'federal',\n",
       " 'feel',\n",
       " 'field',\n",
       " 'figure',\n",
       " 'file',\n",
       " 'files',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'fine',\n",
       " 'firearms',\n",
       " 'flight',\n",
       " 'floppy',\n",
       " 'folks',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'font',\n",
       " 'food',\n",
       " 'force',\n",
       " 'forget',\n",
       " 'form',\n",
       " 'format',\n",
       " 'forward',\n",
       " 'frank',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'function',\n",
       " 'functions',\n",
       " 'future',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'generally',\n",
       " 'genocide',\n",
       " 'george',\n",
       " 'germany',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'goal',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'government',\n",
       " 'graphics',\n",
       " 'great',\n",
       " 'greatly',\n",
       " 'greek',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'groups',\n",
       " 'guess',\n",
       " 'guns',\n",
       " 'guys',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'hands',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happens',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hardware',\n",
       " 'head',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heaven',\n",
       " 'held',\n",
       " 'hell',\n",
       " 'help',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'history',\n",
       " 'hockey',\n",
       " 'hold',\n",
       " 'holy',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'host',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'human',\n",
       " 'idea',\n",
       " 'ideas',\n",
       " 'image',\n",
       " 'images',\n",
       " 'imagine',\n",
       " 'important',\n",
       " 'include',\n",
       " 'included',\n",
       " 'includes',\n",
       " 'including',\n",
       " 'increase',\n",
       " 'independent',\n",
       " 'individual',\n",
       " 'info',\n",
       " 'information',\n",
       " 'input',\n",
       " 'inside',\n",
       " 'installed',\n",
       " 'instead',\n",
       " 'institute',\n",
       " 'insurance',\n",
       " 'intended',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interface',\n",
       " 'internal',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'involved',\n",
       " 'israel',\n",
       " 'israeli',\n",
       " 'issue',\n",
       " 'issues',\n",
       " 'james',\n",
       " 'jesus',\n",
       " 'jewish',\n",
       " 'jews',\n",
       " 'jobs',\n",
       " 'john',\n",
       " 'jpeg',\n",
       " 'kept',\n",
       " 'keyboard',\n",
       " 'keys',\n",
       " 'kill',\n",
       " 'killed',\n",
       " 'killing',\n",
       " 'kind',\n",
       " 'king',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knowledge',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'lack',\n",
       " 'land',\n",
       " 'language',\n",
       " 'large',\n",
       " 'late',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'launch',\n",
       " 'laws',\n",
       " 'lead',\n",
       " 'league',\n",
       " 'learn',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'legal',\n",
       " 'letter',\n",
       " 'level',\n",
       " 'library',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'limited',\n",
       " 'line',\n",
       " 'lines',\n",
       " 'list',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lives',\n",
       " 'living',\n",
       " 'local',\n",
       " 'logic',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lord',\n",
       " 'lost',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'lower',\n",
       " 'luck',\n",
       " 'lunar',\n",
       " 'machine',\n",
       " 'machines',\n",
       " 'magazine',\n",
       " 'mail',\n",
       " 'mailing',\n",
       " 'main',\n",
       " 'major',\n",
       " 'majority',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'management',\n",
       " 'manager',\n",
       " 'manual',\n",
       " 'march',\n",
       " 'mark',\n",
       " 'market',\n",
       " 'mass',\n",
       " 'master',\n",
       " 'material',\n",
       " 'matter',\n",
       " 'matthew',\n",
       " 'maybe',\n",
       " 'mean',\n",
       " 'meaning',\n",
       " 'means',\n",
       " 'media',\n",
       " 'medical',\n",
       " 'member',\n",
       " 'members',\n",
       " 'memory',\n",
       " 'mention',\n",
       " 'mentioned',\n",
       " 'message',\n",
       " 'messages',\n",
       " 'method',\n",
       " 'michael',\n",
       " 'middle',\n",
       " 'mike',\n",
       " 'miles',\n",
       " 'military',\n",
       " 'million',\n",
       " 'mind',\n",
       " 'minutes',\n",
       " 'misc',\n",
       " 'mission',\n",
       " 'mode',\n",
       " 'model',\n",
       " 'models',\n",
       " 'modem',\n",
       " 'modern',\n",
       " 'money',\n",
       " 'monitor',\n",
       " 'month',\n",
       " 'months',\n",
       " 'moon',\n",
       " 'moral',\n",
       " 'mother',\n",
       " 'motif',\n",
       " 'mouse',\n",
       " 'multi',\n",
       " 'multiple',\n",
       " 'muslim',\n",
       " 'names',\n",
       " 'nasa',\n",
       " 'national',\n",
       " 'natural',\n",
       " 'nature',\n",
       " 'near',\n",
       " 'necessarily',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'netcom',\n",
       " 'network',\n",
       " 'news',\n",
       " 'newsgroup',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'normal',\n",
       " 'north',\n",
       " 'note',\n",
       " 'null',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'object',\n",
       " 'objective',\n",
       " 'obvious',\n",
       " 'obviously',\n",
       " 'offer',\n",
       " 'offers',\n",
       " 'office',\n",
       " 'official',\n",
       " 'ones',\n",
       " 'open',\n",
       " 'operation',\n",
       " 'opinion',\n",
       " 'opinions',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orbit',\n",
       " 'order',\n",
       " 'organization',\n",
       " 'original',\n",
       " 'output',\n",
       " 'outside',\n",
       " 'package',\n",
       " 'page',\n",
       " 'paid',\n",
       " 'pain',\n",
       " 'paper',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'parts',\n",
       " 'party',\n",
       " 'pass',\n",
       " 'past',\n",
       " 'paul',\n",
       " 'peace',\n",
       " 'people',\n",
       " 'perfect',\n",
       " 'performance',\n",
       " 'period',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'peter',\n",
       " 'phone',\n",
       " 'physical',\n",
       " 'pick',\n",
       " 'picture',\n",
       " 'pittsburgh',\n",
       " 'place',\n",
       " 'places',\n",
       " 'plan',\n",
       " 'play',\n",
       " 'played',\n",
       " 'player',\n",
       " 'players',\n",
       " 'playing',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'points',\n",
       " 'police',\n",
       " 'policy',\n",
       " 'political',\n",
       " 'poor',\n",
       " 'population',\n",
       " 'port',\n",
       " 'position',\n",
       " 'possible',\n",
       " 'possibly',\n",
       " 'post',\n",
       " 'posted',\n",
       " 'posting',\n",
       " 'posts',\n",
       " 'power',\n",
       " 'practice',\n",
       " 'present',\n",
       " 'president',\n",
       " 'press',\n",
       " 'pretty',\n",
       " 'prevent',\n",
       " 'previous',\n",
       " 'price',\n",
       " 'print',\n",
       " 'printer',\n",
       " 'privacy',\n",
       " 'private',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'process',\n",
       " 'processing',\n",
       " 'produce',\n",
       " 'product',\n",
       " 'products',\n",
       " 'program',\n",
       " 'programming',\n",
       " 'programs',\n",
       " 'project',\n",
       " 'protect',\n",
       " 'protection',\n",
       " 'prove',\n",
       " 'provide',\n",
       " 'provided',\n",
       " 'provides',\n",
       " 'public',\n",
       " 'published',\n",
       " 'purpose',\n",
       " 'quality',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quite',\n",
       " 'quote',\n",
       " 'radio',\n",
       " 'range',\n",
       " 'rate',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'real',\n",
       " 'reality',\n",
       " 'realize',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'reasonable',\n",
       " 'reasons',\n",
       " 'receive',\n",
       " 'received',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'record',\n",
       " 'reference',\n",
       " 'references',\n",
       " 'regarding',\n",
       " 'regular',\n",
       " 'related',\n",
       " 'release',\n",
       " 'religion',\n",
       " 'religious',\n",
       " 'remember',\n",
       " 'remote',\n",
       " 'reply',\n",
       " 'report',\n",
       " 'reported',\n",
       " 'reports',\n",
       " 'request',\n",
       " 'require',\n",
       " 'required',\n",
       " 'requires',\n",
       " 'research',\n",
       " 'resource',\n",
       " 'resources',\n",
       " 'respond',\n",
       " 'response',\n",
       " 'responsible',\n",
       " 'rest',\n",
       " 'result',\n",
       " 'results',\n",
       " 'return',\n",
       " 'richard',\n",
       " 'ride',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'risk',\n",
       " 'road',\n",
       " 'robert',\n",
       " 'room',\n",
       " 'round',\n",
       " 'rule',\n",
       " 'rules',\n",
       " 'running',\n",
       " 'runs',\n",
       " 'russia',\n",
       " 'russian',\n",
       " 'safe',\n",
       " 'safety',\n",
       " 'said',\n",
       " 'sale',\n",
       " 'satellite',\n",
       " 'save',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'school',\n",
       " 'science',\n",
       " 'scientific',\n",
       " 'scott',\n",
       " 'screen',\n",
       " 'scsi',\n",
       " 'search',\n",
       " 'season',\n",
       " 'second',\n",
       " 'secret',\n",
       " 'section',\n",
       " 'secure',\n",
       " 'security',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'sell',\n",
       " 'send',\n",
       " 'sense',\n",
       " 'sent',\n",
       " 'serial',\n",
       " 'series',\n",
       " 'seriously',\n",
       " 'server',\n",
       " 'service',\n",
       " 'services',\n",
       " 'shall',\n",
       " 'shell',\n",
       " 'shipping',\n",
       " 'short',\n",
       " 'shot',\n",
       " 'shows',\n",
       " 'shuttle',\n",
       " 'similar',\n",
       " 'simple',\n",
       " 'simply',\n",
       " 'single',\n",
       " 'site',\n",
       " 'sites',\n",
       " 'situation',\n",
       " 'size',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smith',\n",
       " 'society',\n",
       " 'software',\n",
       " 'sold',\n",
       " 'soldiers',\n",
       " 'solution',\n",
       " 'somebody',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'sound',\n",
       " 'sounds',\n",
       " 'source',\n",
       " 'sources',\n",
       " 'south',\n",
       " 'soviet',\n",
       " 'space',\n",
       " 'speak',\n",
       " 'special',\n",
       " 'specific',\n",
       " 'speed',\n",
       " 'spirit',\n",
       " 'stand',\n",
       " 'standard',\n",
       " 'standards',\n",
       " 'start',\n",
       " 'started',\n",
       " 'state',\n",
       " 'stated',\n",
       " 'statement',\n",
       " 'states',\n",
       " 'station',\n",
       " 'stay',\n",
       " 'step',\n",
       " 'stephanopoulos',\n",
       " 'steve',\n",
       " 'stop',\n",
       " 'story',\n",
       " 'stream',\n",
       " 'street',\n",
       " 'strong',\n",
       " 'student',\n",
       " 'studies',\n",
       " 'study',\n",
       " 'stuff',\n",
       " 'stupid',\n",
       " 'subject',\n",
       " 'suggest',\n",
       " 'suggestions',\n",
       " 'summer',\n",
       " 'supply',\n",
       " 'support',\n",
       " 'supported',\n",
       " 'supports',\n",
       " 'suppose',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'surface',\n",
       " 'suspect',\n",
       " 'switch',\n",
       " 'systems',\n",
       " 'table',\n",
       " 'taken',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tape',\n",
       " 'team',\n",
       " 'teams',\n",
       " 'technical',\n",
       " 'technology',\n",
       " 'tell',\n",
       " 'term',\n",
       " 'terms',\n",
       " 'test',\n",
       " 'texas',\n",
       " 'text',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'theory',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'thomas',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'times',\n",
       " 'title',\n",
       " 'today',\n",
       " 'told',\n",
       " 'took',\n",
       " 'tools',\n",
       " 'toronto',\n",
       " 'total',\n",
       " 'town',\n",
       " 'trade',\n",
       " 'traffic',\n",
       " 'transfer',\n",
       " 'tried',\n",
       " 'trouble',\n",
       " 'true',\n",
       " 'trust',\n",
       " 'truth',\n",
       " 'trying',\n",
       " 'turkey',\n",
       " 'turkish',\n",
       " 'turks',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'type',\n",
       " 'types',\n",
       " 'understand',\n",
       " 'understanding',\n",
       " 'unfortunately',\n",
       " 'unit',\n",
       " 'united',\n",
       " 'universe',\n",
       " 'university',\n",
       " 'unix',\n",
       " 'unless',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'usenet',\n",
       " 'user',\n",
       " 'users',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'uucp',\n",
       " 'uunet',\n",
       " 'value',\n",
       " 'values',\n",
       " 'vancouver',\n",
       " 'various',\n",
       " 'vehicle',\n",
       " 'version',\n",
       " 'versions',\n",
       " 'video',\n",
       " 'view',\n",
       " 'views',\n",
       " 'visual',\n",
       " 'voice',\n",
       " 'volume',\n",
       " 'wait',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wants',\n",
       " 'washington',\n",
       " 'watch',\n",
       " 'water',\n",
       " 'ways',\n",
       " 'weapons',\n",
       " 'week',\n",
       " 'weeks',\n",
       " 'went',\n",
       " 'west',\n",
       " 'western',\n",
       " 'white',\n",
       " 'wide',\n",
       " 'widget',\n",
       " 'wife',\n",
       " 'willing',\n",
       " 'window',\n",
       " 'windows',\n",
       " 'wire',\n",
       " 'wish',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'wonder',\n",
       " 'wondering',\n",
       " 'word',\n",
       " 'words',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worse',\n",
       " 'worth',\n",
       " 'write',\n",
       " 'writing',\n",
       " 'written',\n",
       " 'wrong',\n",
       " 'wrote',\n",
       " 'xterm',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'years',\n",
       " 'york',\n",
       " 'young']"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = vector.get_feature_names()  # 1000개의 단어 리스트\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토픽 1 :  [('like', 0.205), ('know', 0.188), ('people', 0.184), ('think', 0.168), ('good', 0.143), ('time', 0.139), ('thanks', 0.121), ('make', 0.104), ('right', 0.103), ('want', 0.1)]\n",
      "\n",
      "토픽 2 :  [('thanks', 0.338), ('windows', 0.275), ('mail', 0.177), ('card', 0.171), ('drive', 0.156), ('file', 0.134), ('advance', 0.131), ('email', 0.121), ('software', 0.112), ('program', 0.106)]\n",
      "\n",
      "토픽 3 :  [('game', 0.381), ('team', 0.324), ('year', 0.273), ('games', 0.245), ('season', 0.187), ('hockey', 0.172), ('players', 0.166), ('play', 0.156), ('good', 0.132), ('league', 0.121)]\n",
      "\n",
      "토픽 4 :  [('drive', 0.513), ('scsi', 0.203), ('disk', 0.156), ('hard', 0.155), ('card', 0.145), ('drives', 0.141), ('problem', 0.12), ('controller', 0.104), ('apple', 0.101), ('floppy', 0.1)]\n",
      "\n",
      "토픽 5 :  [('thanks', 0.371), ('drive', 0.358), ('know', 0.265), ('scsi', 0.136), ('advance', 0.123), ('jesus', 0.117), ('people', 0.111), ('mail', 0.104), ('drives', 0.103), ('hard', 0.092)]\n",
      "\n",
      "토픽 6 :  [('windows', 0.365), ('know', 0.226), ('think', 0.184), ('like', 0.183), ('file', 0.142), ('window', 0.141), ('problem', 0.111), ('jesus', 0.106), ('files', 0.094), ('card', 0.087)]\n",
      "\n",
      "토픽 7 :  [('like', 0.549), ('know', 0.189), ('bike', 0.168), ('chip', 0.129), ('clipper', 0.083), ('sounds', 0.079), ('work', 0.078), ('space', 0.077), ('encryption', 0.073), ('looks', 0.067)]\n",
      "\n",
      "토픽 8 :  [('know', 0.237), ('thanks', 0.223), ('government', 0.205), ('people', 0.176), ('drive', 0.138), ('right', 0.136), ('israel', 0.125), ('armenian', 0.124), ('chip', 0.116), ('game', 0.112)]\n",
      "\n",
      "토픽 9 :  [('card', 0.491), ('video', 0.234), ('monitor', 0.154), ('sale', 0.152), ('price', 0.126), ('people', 0.123), ('drivers', 0.122), ('offer', 0.113), ('cards', 0.111), ('driver', 0.085)]\n",
      "\n",
      "토픽 10 :  [('like', 0.5), ('people', 0.239), ('windows', 0.214), ('armenian', 0.169), ('israel', 0.162), ('turkish', 0.153), ('armenians', 0.145), ('drive', 0.112), ('armenia', 0.099), ('university', 0.098)]\n",
      "\n",
      "토픽 11 :  [('think', 0.486), ('internet', 0.224), ('university', 0.214), ('card', 0.135), ('know', 0.131), ('uucp', 0.116), ('like', 0.115), ('uunet', 0.112), ('computer', 0.101), ('mail', 0.097)]\n",
      "\n",
      "토픽 12 :  [('know', 0.429), ('university', 0.2), ('card', 0.19), ('space', 0.145), ('computer', 0.126), ('israel', 0.125), ('internet', 0.122), ('nasa', 0.102), ('jesus', 0.095), ('said', 0.094)]\n",
      "\n",
      "토픽 13 :  [('like', 0.304), ('chip', 0.243), ('game', 0.213), ('jesus', 0.2), ('encryption', 0.164), ('clipper', 0.155), ('keys', 0.119), ('team', 0.117), ('government', 0.108), ('john', 0.104)]\n",
      "\n",
      "토픽 14 :  [('think', 0.359), ('thanks', 0.24), ('space', 0.155), ('work', 0.129), ('card', 0.129), ('time', 0.124), ('game', 0.122), ('israel', 0.099), ('like', 0.091), ('nasa', 0.089)]\n",
      "\n",
      "토픽 15 :  [('space', 0.293), ('people', 0.287), ('know', 0.218), ('nasa', 0.17), ('file', 0.167), ('card', 0.131), ('files', 0.099), ('list', 0.098), ('program', 0.091), ('sale', 0.088)]\n",
      "\n",
      "토픽 16 :  [('israel', 0.404), ('israeli', 0.195), ('good', 0.195), ('file', 0.15), ('think', 0.133), ('jews', 0.127), ('year', 0.118), ('files', 0.111), ('arab', 0.104), ('know', 0.099)]\n",
      "\n",
      "토픽 17 :  [('mail', 0.478), ('right', 0.243), ('time', 0.217), ('know', 0.193), ('address', 0.117), ('israel', 0.112), ('problem', 0.109), ('think', 0.096), ('send', 0.094), ('post', 0.087)]\n",
      "\n",
      "토픽 18 :  [('good', 0.359), ('time', 0.337), ('mail', 0.284), ('israel', 0.123), ('like', 0.111), ('software', 0.094), ('apple', 0.091), ('people', 0.087), ('steve', 0.075), ('question', 0.074)]\n",
      "\n",
      "토픽 19 :  [('armenian', 0.221), ('turkish', 0.2), ('good', 0.192), ('think', 0.185), ('armenians', 0.183), ('said', 0.165), ('university', 0.161), ('know', 0.147), ('work', 0.14), ('chip', 0.137)]\n",
      "\n",
      "토픽 20 :  [('time', 0.423), ('year', 0.26), ('windows', 0.246), ('thanks', 0.174), ('jesus', 0.161), ('years', 0.14), ('chip', 0.129), ('university', 0.098), ('encryption', 0.09), ('clipper', 0.088)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getTopic(c, fName, n = 10):\n",
    "    \n",
    "    for i, t in enumerate(c):\n",
    "        print('토픽 %d : '%(i+1), [(fName[i], t[i].round(3)) for i in t.argsort()[:-n-1:-1]], end='\\n\\n')\n",
    "    \n",
    "    \n",
    "getTopic(svdModel.components_, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlow : 5\\nlower : 2\\nnewest : 6\\nwidest : 3\\n'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BPE 알고리즘 구현\n",
    "'''\n",
    "low : 5\n",
    "lower : 2\n",
    "newest : 6\n",
    "widest : 3\n",
    "'''\n",
    "#   l, o, w, e, r, n, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l o w': 5, 'l o w e r': 2, 'n e w e s t': 6, 'w i d e s t': 3}\n",
      "['l', 'r', 's', 'o', 'w', 'n', 'd', 'e', 't', 'i']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "data = {'low' : 5,\n",
    "'lower' : 2,\n",
    "'newest' : 6,\n",
    "'widest' : 3}\n",
    "\n",
    "# 첫 VOC\n",
    "voc = list(set(''.join([term for term, time in data.items()])))\n",
    "\n",
    "data2 = {}\n",
    "for term, time in data.items():\n",
    "    temp = []\n",
    "    \n",
    "    for j in term:\n",
    "        temp.append(j)\n",
    "    temp = ' '.join(temp)\n",
    "    data2[str(temp)] = time\n",
    "    \n",
    "print(data2)\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l r'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(['l','r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l r\n",
      "l s\n",
      "l o\n",
      "l w\n",
      "l n\n",
      "l d\n",
      "l e\n",
      "l t\n",
      "l i\n",
      "r l\n",
      "r s\n",
      "r o\n",
      "r w\n",
      "r n\n",
      "r d\n",
      "r e\n",
      "r t\n",
      "r i\n",
      "s l\n",
      "s r\n",
      "s o\n",
      "s w\n",
      "s n\n",
      "s d\n",
      "s e\n",
      "s t\n",
      "s i\n",
      "o l\n",
      "o r\n",
      "o s\n",
      "o w\n",
      "o n\n",
      "o d\n",
      "o e\n",
      "o t\n",
      "o i\n",
      "w l\n",
      "w r\n",
      "w s\n",
      "w o\n",
      "w n\n",
      "w d\n",
      "w e\n",
      "w t\n",
      "w i\n",
      "n l\n",
      "n r\n",
      "n s\n",
      "n o\n",
      "n w\n",
      "n d\n",
      "n e\n",
      "n t\n",
      "n i\n",
      "d l\n",
      "d r\n",
      "d s\n",
      "d o\n",
      "d w\n",
      "d n\n",
      "d e\n",
      "d t\n",
      "d i\n",
      "e l\n",
      "e r\n",
      "e s\n",
      "e o\n",
      "e w\n",
      "e n\n",
      "e d\n",
      "e t\n",
      "e i\n",
      "t l\n",
      "t r\n",
      "t s\n",
      "t o\n",
      "t w\n",
      "t n\n",
      "t d\n",
      "t e\n",
      "t i\n",
      "i l\n",
      "i r\n",
      "i s\n",
      "i o\n",
      "i w\n",
      "i n\n",
      "i d\n",
      "i e\n",
      "i t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['l', 'r', 's', 'o', 'w', 'n', 'd', 'e', 't', 'i']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "max_cnt = 0\n",
    "max_pair = ''\n",
    "\n",
    "for i in voc:\n",
    "    for j in voc:\n",
    "        if i != j:\n",
    "            pair = ' '.join([i,j])\n",
    "            cnt = 0\n",
    "            print(pair)\n",
    "            for check in [x for x in data2]:\n",
    "                temp_cnt = 0\n",
    "                if re.search(pair, check):\n",
    "                    temp_cnt += 1\n",
    "                    temp_cnt *= data2[check]\n",
    "                    cnt += temp_cnt\n",
    "\n",
    "#             if cnt > max_cnt:\n",
    "#                 max_cnt = cnt\n",
    "#                 max_pair = pair\n",
    "            \n",
    "#         change = max_pair.replace(' ','')\n",
    "#         print(change)\n",
    "#         voc.append(change)\n",
    "\n",
    "#         # key값의 공백 제거\n",
    "#         for i in [x for x in data2]:\n",
    "#             temp_key = i.replace(max_pair, change)\n",
    "#     #                 print(temp_key)\n",
    "#             data2[temp_key] = data2.pop(i)\n",
    "\n",
    "\n",
    "\n",
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'o', 'l', 't', 'e', 'n', 's', 'i', 'w', 'r']"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
